{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#_1","title":"Home","text":"<p>LazyQML is a Python library designed to streamline, automate, and accelerate experimentation with Quantum Machine Learning (QML) architectures, right on classical computers.</p> <p>With LazyQML, you can:   - \ud83d\udee0\ufe0f Build, test, and benchmark QML models with minimal effort.</p> <ul> <li> <p>\u26a1 Compare different QML architectures, hyperparameters seamlessly.</p> </li> <li> <p>\ud83e\udde0 Gather knowledge about the most suitable architecture for your problem.</p> </li> </ul>"},{"location":"#why-lazyqml","title":"\u2728 Why LazyQML?","text":"<ul> <li> <p>Rapid Prototyping: Experiment with different QML models using just a few lines of code.</p> </li> <li> <p>Automated Benchmarking: Evaluate performance and trade-offs across architectures effortlessly.</p> </li> <li> <p>Flexible &amp; Modular: From basic quantum circuits to hybrid quantum-classical models\u2014LazyQML has you covered.</p> </li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>For detailed usage instructions, API reference, and code examples, please refer to the official LazyQML documentation.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.10</li> </ul> <p>\u2757\u2757  This library is only supported by Linux Systems. It doesn't support Windows nor MacOS.  Only supports CUDA compatible devices.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install lazyqml, run this command in your terminal:</p> <pre><code>pip install lazyqml\n</code></pre> <p>This is the preferred method to install lazyqml, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"#from-sources","title":"From sources","text":"<p>To install lazyqml from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/QHPC-SP-Research-Lab/LazyQML\n</code></pre>"},{"location":"#example","title":"Example","text":"<pre><code>from sklearn.datasets import load_iris\nfrom lazyqml import QuantumClassifier\n\n# Load data\ndata = load_iris()\nX = data.data\ny = data.target\n\nclassifier = QuantumClassifier(nqubits={4}, classifiers={Model.QNN, Model.QSVM}, epochs=10)\n\n# Fit and predict\nclassifier.fit(X=X, y=y, test_size=0.4)\n</code></pre>"},{"location":"#quantum-and-high-performance-computing-qhpc-university-of-oviedo","title":"Quantum and High Performance Computing (QHPC) - University of Oviedo","text":"<ul> <li>Jos\u00e9 Ranilla Pastor - ranilla@uniovi.es</li> <li>El\u00edas Fern\u00e1ndez Combarro - efernandezca@uniovi.es</li> <li>Diego Garc\u00eda Vega - diegogarciavega@gmail.com</li> <li>Fernando \u00c1lvaro Plou Llorente - ploufernando@uniovi.es</li> <li>Alejandro Leal Casta\u00f1o - lealcalejandro@uniovi.es</li> <li>Group - https://qhpc.uniovi.es</li> </ul>"},{"location":"#citing","title":"Citing","text":"<p>If you used LazyQML in your work, please cite: - Garc\u00eda-Vega, D., Plou Llorente, F., Leal Casta\u00f1o, A., Combarro, E.F., Ranilla, J.: Lazyqml: A python library to benchmark quantum machine learning models. In: 30th European Conference on Parallel and Distributed Processing (2024)</p>"},{"location":"#license","title":"License","text":"<ul> <li>Free software: MIT License</li> </ul>"},{"location":"api/","title":"LazyQML API Overview","text":"<p>Welcome to LazyQML \u2013 your quantum machine learning playground! LazyQML is a cutting-edge Python library designed to simplify the integration of quantum classifiers into your machine learning workflows. With LazyQML, you'll be able to explore quantum neural networks, quantum support vector machines, and other quantum models, all while maintaining a simple and easy to use code.</p> <p>At the heart of LazyQML is the QuantumClassifier \u2013 the Swiss Army knife of quantum machine learning. This easy-to-use class empowers you to train, evaluate, and fine-tune quantum classifiers on your data, whether you're a beginner or a seasoned quantum enthusiast. </p>"},{"location":"api/#key-features","title":"Key Features","text":"<p>LazyQML is packed with tools to streamline quantum classification. Below are the core features that set it apart from the crowd:</p>"},{"location":"api/#1-quantumclassifier-the-heart-of-lazyqml","title":"1. QuantumClassifier: The Heart of LazyQML","text":"<p>The QuantumClassifier class is the core of LazyQML, offering a variety of methods for training and evaluating quantum models. It provides an elegant and flexible interface for working with quantum circuits, allowing you to explore different types of classifiers, embeddings, and ansatz circuits. The goal? To make quantum classification as intuitive as possible. </p>"},{"location":"api/#2-simulation-variants","title":"2. Simulation variants","text":"<p>LazyQML provides two simulation types, depending on the qubit representation used underneath. This gives you the freedom to choose the right quantum simulation backend for your specific needs:</p> <ul> <li> <p>State Vector Simulation: This variant simulates the full quantum state of your system, perfect for smaller systems or when you want a more intuitive understanding of quantum behavior.</p> </li> <li> <p>Tensor Networks: This variant uses tensor networks (MPS), providing higher scalability for larger quantum systems. It's optimized for more complex and larger datasets, helping you tackle big problems with ease.</p> </li> </ul>"},{"location":"api/#selecting-quantumclassifier-variant","title":"Selecting QuantumClassifier variant","text":"<p>Choosing the type of simulation is as simple as calling the <code>set_simulation_type</code> method with the appropiate string: <code>'statevector'</code> for the state vector simulation and <code>'tensor'</code> for the tensor network variation.</p> <p>We also offer methods to change the maximum bond dimension for the <code>'tensor'</code> representation, in order to fine-tune and provide more control of the resulting QML models.</p> <pre><code>from lazyqml.Utils import set_simulation_type, set_max_bond_dim\n\n# Use tensor network qubit representation\nset_simulation_type('tensor')\nset_max_bond_dim(32)\n\n# Use state vector qubit representation\nset_simulation_type('statevector')\n</code></pre>"},{"location":"api/#3-training-and-evaluation-methods","title":"3. Training and Evaluation Methods","text":"<p>LazyQML offers you three robust methods to train and evaluate your quantum models. These methods are designed to give you complete control over the classification process:</p>"},{"location":"api/#quantumclassifierfit","title":"<code>QuantumClassifier.fit(...)</code>","text":"<p>The fit method is where the magic happens. It trains your quantum model on your dataset, selecting from different quantum classifiers, embeddings, and ansatz circuits. This method provides a simple interface to quickly train a model, view its results, and get on with your quantum journey.</p> <ul> <li>When to use it? Use <code>fit</code> when you want to quickly train and evaluate a quantum model with just a few lines of code.</li> </ul>"},{"location":"api/#quantumclassifierrepeated_cross_validation","title":"<code>QuantumClassifier.repeated_cross_validation(...)</code>","text":"<p>This method performs repeated k-fold cross-validation. It divides your dataset into k subsets, trains the model on k-1 subsets, and tests on the remaining fold. This process is repeated multiple times to provide a more accurate estimate of your model's performance.</p> <ul> <li>When to use it? Use <code>repeated_cross_validation</code> for a more comprehensive evaluation of your model, especially when working with larger datasets.</li> </ul>"},{"location":"api/#quantumclassifierleave_one_out","title":"<code>QuantumClassifier.leave_one_out(...)</code>","text":"<p>Leave-One-Out Cross Validation (LOOCV) is a robust technique where each data point is used as the test set exactly once. This method is fantastic for small datasets, providing a deeper understanding of your model\u2019s performance.</p> <ul> <li>When to use it? Choose <code>leave_one_out</code> when working with small datasets and you need to evaluate every data point for a thorough assessment.</li> </ul>"},{"location":"api/#4-quantum-model-selection","title":"4. Quantum Model Selection","text":"<p>LazyQML gives you full control over your quantum model's architecture. With a rich set of predefined enums, you can easily select the correct ansatz circuits, embedding strategies, and classification models.</p>"},{"location":"api/#ansatzs","title":"<code>Ansatzs</code>","text":"<p>Ansatz circuits set the core structure of your QNN models, defining the trainable parameters that allows learning from data. LazyQML provides a readily selection of ansatz types:</p> <ul> <li><code>ALL</code>: All available ansatz circuits.</li> <li><code>HCZRX</code>, <code>TREE_TENSOR</code>, <code>TWO_LOCAL</code>, <code>HARDWARE_EFFICIENT</code>, <code>ANNULAR</code>: Popular ansatz circuits that are ideal for quantum machine learning.</li> </ul>"},{"location":"api/#embeddings","title":"<code>Embeddings</code>","text":"<p>Embeddings control how your classical data is encoded onto quantum states. LazyQML offers several types of embedding strategies:</p> <ul> <li><code>ALL</code>: All available embedding circuits.</li> <li><code>RX</code>, <code>RY</code>, <code>RZ</code>: Common qubit rotation embeddings.</li> <li><code>ZZ</code>, <code>AMP</code>, <code>DENSE_ANGLE</code>, <code>HIGHER_ORDER</code>: Embedding strategies based on entanglement and/or amplitude encoding.</li> </ul>"},{"location":"api/#model","title":"<code>Model</code>","text":"<p>LazyQML supports a variety of quantum models, each suited for different tasks. Choose the model that best fits your data and problem:</p> <ul> <li><code>ALL</code>: All available quantum models.</li> <li><code>QNN</code>: Quantum Neural Network.</li> <li><code>QNN_BAG</code>: Quantum Neural Network with Bagging.</li> <li><code>QSVM</code>: Quantum Support Vector Machine.</li> <li><code>QKNN</code>: Quantum k-Nearest Neighbors.</li> </ul>"},{"location":"api/#whats-next","title":"What's Next?","text":"<p>This overview introduces you to the powerful features of LazyQML and the QuantumClassifier. Whether you\u2019re just getting started or you\u2019re a quantum computing pro, LazyQML simplifies quantum machine learning.</p> <p>For more detailed documentation on each function, parameter, and quantum algorithm, head over to the full documentation pages. Get ready to dive into the world of quantum classification with LazyQML \u2013 your quantum adventure begins here!</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"common/","title":"common module","text":""},{"location":"common/#lazyqml.lazyqml.QuantumClassifier","title":"<code> QuantumClassifier            (BaseModel)         </code>","text":"<p>Main class of lazyqml that serves as an inteface to build and train a wide variety of quantum machine learning models with little setup. It stores model configurations and and functions as a starting point for model fitting.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier--parameters","title":"Parameters","text":"<p>nqubits : set of ints     Set of qubits to be used in the circuits of the quantum models randomSate : int, optional (default=1234)     This integer is used as a seed for the repeatability of the experiments. ignoreWarnings : bool, optional (default=True)     When set to True, the warning related to algorithms that are not able to run are ignored. sequential : bool, optional (default=False)     If set to True, executes selected models and circuits in a sequential manner. Otherwise, they are executed in parallel. numPredictors : int, optional (default=10)     The number of different predictoras that the Quantum Neural Networks with Bagging (QNN_Bag) will use. numLayers : int, optional (default=5)     The number of layers that the QNN models will use. classifiers : set of Model enums, optional (default={Model.ALL})     Selects the quantum models to build and train. Possible values are: Model.ALL, Model.QNN, Model.QNN_BAG and Model.QSVM ansatzs : set of Ansatzs enums, optional (default={Ansatzs.ALL})     Selects the ansatzs to build the QNN and QNNBag quantum models. Possible values are: Ansatzs.ALL, Ansatzs.HCZRX, Ansatzs.TREE_TENSOR, Ansatzs.TWO_LOCAL, Ansatzs.HARDWARE_EFFICIENT, Ansatzs.ANNULAR. embeddings : list of strings, optional (default={Embedding.ALL})     Selects the embeddings for all available quantum models. Possible values are: Embedding.ALL, Embedding.RX, Embedding.RY, Embedding.RZ, Embedding.ZZ, Embedding.AMP, Embedding.DENSE_ANGLE, Embedding.HIGHER_ORDER. features : set of floats, optional (default={0.3, 0.5, 0.8})     Set of floating point numbers between 0 and 1.0 that indicates the percentage of data features to be used for each predictor in the QNNBag quantum model. For each value, a new QNNBag model will be trained. learningRate : int, optional (default=0.01)     The parameter that will be used for the optimization process of all the QNN and QNNBag models in the gradient descent. epochs : int, optional (default=100)     Number of complete passes that will be done over the dataset while fitting the models. batchSize : int, optional (default=8)     Number of datapoints per batch when training QNN and QNNBag models. threshold : int, optional (default=22)     This parameter partially determines when to use GPU over CPU. If number of qubits surpases this threshold, GPU execution will be prioritized over CPU, but its not guaranteed. Only used for QNN models. maxSamples : float, optional (default=1.0)     A floating point number between 0 and 1.0 that indicates the percentage of the dataset that will be used for each predictor in the QNNBag quantum model. verbose : bool, optional (default=False)     If True, shows all training messages during the fitting of the selected models. customMetric : function, optional (default=None)     When function is provided, models are evaluated based on the custom evaluation metric provided. customImputerNum : function, optional (default=None)     When function is provided, models are imputed based on the custom numeric imputer provided. customImputerCat : function, optional (default=None)     When function is provided, models are imputed based on the custom categorical imputer provided. cores : int, optional (default=-1)     Number of cores used for parallel execution. If cores = -1, maximum cores available in CPU will be used.</p> Source code in <code>lazyqml/lazyqml.py</code> <pre><code>class QuantumClassifier(BaseModel):\n    \"\"\"\n    Main class of lazyqml that serves as an inteface to build and train a wide variety of quantum machine learning models with little setup. It stores model configurations and and functions as a starting point for model fitting.\n\n    Parameters\n    ----------\n    nqubits : set of ints\n        Set of qubits to be used in the circuits of the quantum models\n    randomSate : int, optional (default=1234)\n        This integer is used as a seed for the repeatability of the experiments.\n    ignoreWarnings : bool, optional (default=True)\n        When set to True, the warning related to algorithms that are not able to run are ignored.\n    sequential : bool, optional (default=False)\n        If set to True, executes selected models and circuits in a sequential manner. Otherwise, they are executed in parallel.\n    numPredictors : int, optional (default=10)\n        The number of different predictoras that the Quantum Neural Networks with Bagging (QNN_Bag) will use.\n    numLayers : int, optional (default=5)\n        The number of layers that the QNN models will use.\n    classifiers : set of Model enums, optional (default={Model.ALL})\n        Selects the quantum models to build and train. Possible values are: Model.ALL, Model.QNN, Model.QNN_BAG and Model.QSVM\n    ansatzs : set of Ansatzs enums, optional (default={Ansatzs.ALL})\n        Selects the ansatzs to build the QNN and QNNBag quantum models. Possible values are: Ansatzs.ALL, Ansatzs.HCZRX, Ansatzs.TREE_TENSOR, Ansatzs.TWO_LOCAL, Ansatzs.HARDWARE_EFFICIENT, Ansatzs.ANNULAR.\n    embeddings : list of strings, optional (default={Embedding.ALL})\n        Selects the embeddings for all available quantum models. Possible values are: Embedding.ALL, Embedding.RX, Embedding.RY, Embedding.RZ, Embedding.ZZ, Embedding.AMP, Embedding.DENSE_ANGLE, Embedding.HIGHER_ORDER.\n    features : set of floats, optional (default={0.3, 0.5, 0.8})\n        Set of floating point numbers between 0 and 1.0 that indicates the percentage of data features to be used for each predictor in the QNNBag quantum model. For each value, a new QNNBag model will be trained.\n    learningRate : int, optional (default=0.01)\n        The parameter that will be used for the optimization process of all the QNN and QNNBag models in the gradient descent.\n    epochs : int, optional (default=100)\n        Number of complete passes that will be done over the dataset while fitting the models.\n    batchSize : int, optional (default=8)\n        Number of datapoints per batch when training QNN and QNNBag models.\n    threshold : int, optional (default=22)\n        This parameter partially determines when to use GPU over CPU. If number of qubits surpases this threshold, GPU execution will be prioritized over CPU, but its not guaranteed. Only used for QNN models.\n    maxSamples : float, optional (default=1.0)\n        A floating point number between 0 and 1.0 that indicates the percentage of the dataset that will be used for each predictor in the QNNBag quantum model.\n    verbose : bool, optional (default=False)\n        If True, shows all training messages during the fitting of the selected models.\n    customMetric : function, optional (default=None)\n        When function is provided, models are evaluated based on the custom evaluation metric provided.\n    customImputerNum : function, optional (default=None)\n        When function is provided, models are imputed based on the custom numeric imputer provided.\n    customImputerCat : function, optional (default=None)\n        When function is provided, models are imputed based on the custom categorical imputer provided.\n    cores : int, optional (default=-1)\n        Number of cores used for parallel execution. If cores = -1, maximum cores available in CPU will be used.\n    \"\"\"\n\n    # FIXME: Estos parametros no se usan\n    # runs : int, optional (default=1)\n    #    The number of training runs that will be done with the Quantum Neural Network (QNN) models.\n    # backend : Backend enum (default=Backend.lightningQubit)\n    # shots : int, optional (default=1)\n\n\n    model_config = ConfigDict(strict=True)\n\n    # nqubits: Annotated[int, Field(gt=0)] = 8\n    nqubits: Annotated[Set[int], Field(description=\"Set of qubits, each must be greater than 0\")]\n    randomstate: int = 1234\n    predictions: bool = False\n    ignoreWarnings: bool = True\n    sequential: bool = False\n    numPredictors: Annotated[int, Field(gt=0)] = 10\n    numLayers: Annotated[int, Field(gt=0)] = 5\n    classifiers: Annotated[Set[Model], Field(min_items=1)] = {Model.ALL}\n    ansatzs: Annotated[Set[Ansatzs], Field(min_items=1)] = {Ansatzs.ALL}\n    embeddings: Annotated[Set[Embedding], Field(min_items=1)] = {Embedding.ALL}\n    backend: Backend = Backend.lightningQubit\n    learningRate: Annotated[float, Field(gt=0)] = 0.01\n    epochs: Annotated[int, Field(gt=0)] = 100\n    shots: Annotated[int, Field(gt=0)] = 1\n    runs: Annotated[int, Field(gt=0)] = 1\n    batchSize: Annotated[int, Field(gt=0)] = 8\n    threshold: Annotated[int, Field(gt=0)] = 22\n    numSamples: Annotated[float, Field(gt=0, le=1)] = 1.0\n    numFeatures: Annotated[Set[float], Field(min_items=1)] = {0.3, 0.5, 0.8}\n    verbose: bool = False\n    customMetric: Optional[Callable] = None\n    customImputerNum: Optional[Any] = None\n    customImputerCat: Optional[Any] = None\n    cores: Optional[int] = -1\n    _dispatcher: Any = None\n\n    @field_validator('nqubits', mode='before')\n    def check_nqubits_positive(cls, value):\n        # TODO: Funciona aunque el set no sea de enteros?\n        if not isinstance(value, set):\n            raise TypeError('nqubits must be a set of integers')\n\n        if any(v &lt;= 0 for v in value):\n            raise ValueError('Each value in nqubits must be greater than 0')\n\n        return value\n\n    @field_validator('numFeatures')\n    def validate_features(cls, v):\n        if not all(0 &lt; x &lt;= 1 for x in v):\n            raise ValueError(\"All features must be greater than 0 and less than or equal to 1\")\n        return v\n\n    @field_validator('customMetric')\n    def validate_custom_metric_field(cls, metric):\n        if metric is None:\n            return None  # Allow None as a valid value\n\n        # Check the function signature\n        sig = inspect.signature(metric)\n        params = list(sig.parameters.values())\n\n        if len(params) &lt; 2 or params[0].name != 'y_true' or params[1].name != 'y_pred':\n            raise ValueError(\n                f\"Function {metric.__name__} does not have the required signature. \"\n                f\"Expected first two arguments to be 'y_true' and 'y_pred'.\"\n            )\n\n        # Test the function by passing dummy arguments\n        y_true = np.array([0, 1, 1, 0])  # Example ground truth labels\n        y_pred = np.array([0, 1, 0, 0])  # Example predicted labels\n\n        try:\n            result = metric(y_true, y_pred)\n        except Exception as e:\n            raise ValueError(f\"Function {metric.__name__} raised an error during execution: {e}\")\n\n        # Ensure the result is a scalar (int or float)\n        if not isinstance(result, (int, float)):\n            raise ValueError(\n                f\"Function {metric.__name__} returned {result}, which is not a scalar value.\"\n            )\n\n        return metric\n\n    @field_validator('customImputerCat', 'customImputerNum')\n    def check_preprocessor_methods(cls, preprocessor):\n        # Check if preprocessor is an instance of a class\n        if not isinstance(preprocessor, object):\n            raise ValueError(\n                f\"Expected an instance of a class, but got {type(preprocessor).__name__}.\"\n            )\n\n        # Ensure the object has 'fit' and 'transform' methods\n        if not (hasattr(preprocessor, 'fit') and hasattr(preprocessor, 'transform')):\n            raise ValueError(\n                f\"Object {preprocessor.__class__.__name__} does not have required methods 'fit' and 'transform'.\"\n            )\n\n        # Optionally check if the object has 'fit_transform' method\n        if not hasattr(preprocessor, 'fit_transform'):\n            raise ValueError(\n                f\"Object {preprocessor.__class__.__name__} does not have 'fit_transform' method.\"\n            )\n\n        # Create dummy data for testing the preprocessor methods\n        X_dummy = np.array([[1, 2], [3, 4], [5, 6]])  # Example dummy data\n\n        try:\n            # Ensure the object can fit on data\n            preprocessor.fit(X_dummy)\n        except Exception as e:\n            raise ValueError(f\"Object {preprocessor.__class__.__name__} failed to fit: {e}\")\n\n        try:\n            # Ensure the object can transform data\n            transformed = preprocessor.transform(X_dummy)\n        except Exception as e:\n            raise ValueError(f\"Object {preprocessor.__class__.__name__} failed to transform: {e}\")\n\n        # Check the type of the transformed result\n        if not isinstance(transformed, (np.ndarray, list)):\n            raise ValueError(\n                f\"Object {preprocessor.__class__.__name__} returned {type(transformed)} from 'transform', expected np.ndarray or list.\"\n            )\n\n        return preprocessor\n\n    def model_post_init(self, ctx):\n        self._dispatcher = Dispatcher(\n            sequential=self.sequential,\n            threshold=self.threshold,\n            cores=self.cores,\n            randomstate=self.randomstate,\n            nqubits=self.nqubits,\n            predictions=self.predictions,\n            numPredictors=self.numPredictors,\n            numLayers=self.numLayers,\n            classifiers=self.classifiers,\n            ansatzs=self.ansatzs,\n            backend=self.backend,\n            embeddings=self.embeddings,\n            learningRate=self.learningRate,\n            epochs=self.epochs,\n            runs=self.runs,\n            numSamples=self.numSamples,\n            numFeatures=self.numFeatures,\n            customMetric=self.customMetric,\n            customImputerNum=self.customImputerNum,\n            customImputerCat=self.customImputerCat,\n            shots=self.shots,\n            batch=self.batchSize\n        )\n\n\n    def _prepare_execution(self, X, y):\n        warnings.filterwarnings(\"ignore\")\n        printer.set_verbose(verbose=self.verbose)\n        # Validation model to ensure input parameters are DataFrames and sizes match\n        FitParamsValidatorCV(\n            x=X,\n            y=y\n        )\n        printer.print(\"Validation successful, fitting the model...\")\n\n        # Fix seed\n        fixSeed(self.randomstate)\n\n    def fit(self, X, y, test_size=0.4, showTable=True):\n        \"\"\"\n        Main method of the QuantumClassifier class. Divides the input dataset in train and test according to the test_size parameter, creates and builds all the quantum models using the previously introduced parameters and trains them using X as training datapoints and y as target tags. \n\n        Parameters\n        ----------\n        X : ndarray\n            Complete dataset values to be trained and fitted from.\n        y : ndarray\n            Target tags for each dataset point for supervised learning.\n        test_size : float, optional (default=0.4)\n            Floating point number between 0 and 1.0 that indicates which proportion of the dataset to be used to test the trained models.\n        showTable : bool, optional (default=True)\n            If True, prints the table of results and accuracies in the terminal.\n        \"\"\"\n\n        self._prepare_execution(X, y)\n\n        scores = self._dispatcher.dispatch(\n                        X=X,\n                        y=y,\n                        folds=1,\n                        repeats=1,\n                        mode=\"hold-out\",\n                        testsize=test_size,\n                        showTable=showTable\n                    )\n\n        return scores\n\n    def repeated_cross_validation(self, X, y, n_splits=10, n_repeats=5, showTable=True):\n        \"\"\"\n        Carries out k-fold cross validation based on n_splits (folds) and n_repeats (repeats). \n\n        Parameters\n        ----------\n        X : ndarray\n            Complete dataset values to be trained and fitted from.\n        y : ndarray\n            Target tags for each dataset point for supervised learning.\n        n_splits : int, optional (default=10)\n            Number of folds for k-fold cross validation training.\n        n_repeats : int, optional (default=5)\n            Number of repetitions for k-fold cross validation.\n        showTable : bool, optional (default=True)\n            If True, prints the table of results and accuracies in the terminal.\n        \"\"\"\n        self._prepare_execution(X, y)\n\n        scores = self._dispatcher.dispatch(\n                        X=X,\n                        y=y,\n                        folds=n_splits,\n                        repeats=n_repeats,\n                        mode=\"cross-validation\",\n                        showTable=showTable\n                    )\n\n        return scores\n\n    def leave_one_out(self, X, y, showTable=True):\n        \"\"\"\n        Similar method to repeated_cross_validation. Carries out leave-one-out cross validation. Equivalent to repeated_cross_validation using n_splits=len(X) and n_repeats=1. \n\n        Parameters\n        ----------\n        X : ndarray\n            Complete dataset values to be trained and fitted from.\n        y : ndarray\n            Target tags for each dataset point for supervised learning.\n        n_splits : int, optional (default=10)\n            Number of folds for k-fold cross validation training.\n        n_repeats : int, optional (default=5)\n            Number of repetitions for k-fold cross validation.\n        showTable : bool, optional (default=True)\n            If True, prints the table of results and accuracies in the terminal.\n        \"\"\"\n        self._prepare_execution(X, y)\n\n        scores = self._dispatcher.dispatch(\n                        X=X,\n                        y=y,\n                        folds=len(X),\n                        repeats=1,\n                        mode=\"leave-one-out\",\n                        showTable=showTable\n                    )\n\n        # No funcionaria porque hay que poner el modo dentro del dispatch\n        # self.repeated_cross_validation(X, y, len(X), 1, showTable)\n\n        return scores\n</code></pre>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__class_vars__","title":"<code>__class_vars__</code>  <code>special</code>","text":"<p>The names of the class variables defined on the model.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__private_attributes__","title":"<code>__private_attributes__</code>  <code>special</code>","text":"<p>Metadata about the private attributes of the model.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__pydantic_complete__","title":"<code>__pydantic_complete__</code>  <code>special</code>","text":"<p>Whether model building is completed, or if there are still undefined fields.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__pydantic_computed_fields__","title":"<code>__pydantic_computed_fields__</code>  <code>special</code>","text":"<p>A dictionary of computed field names and their corresponding [<code>ComputedFieldInfo</code>][pydantic.fields.ComputedFieldInfo] objects.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__pydantic_custom_init__","title":"<code>__pydantic_custom_init__</code>  <code>special</code>","text":"<p>Whether the model has a custom <code>__init__</code> method.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__pydantic_decorators__","title":"<code>__pydantic_decorators__</code>  <code>special</code>","text":"<p>Metadata containing the decorators defined on the model. This replaces <code>Model.__validators__</code> and <code>Model.__root_validators__</code> from Pydantic V1.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__pydantic_fields__","title":"<code>__pydantic_fields__</code>  <code>special</code>","text":"<p>A dictionary of field names and their corresponding [<code>FieldInfo</code>][pydantic.fields.FieldInfo] objects. This replaces <code>Model.__fields__</code> from Pydantic V1.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__pydantic_generic_metadata__","title":"<code>__pydantic_generic_metadata__</code>  <code>special</code>","text":"<p>Metadata for generic models; contains data used for a similar purpose to args, origin, parameters in typing-module generics. May eventually be replaced by these.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__pydantic_parent_namespace__","title":"<code>__pydantic_parent_namespace__</code>  <code>special</code>","text":"<p>Parent namespace of the model, used for automatic rebuilding of models.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__pydantic_post_init__","title":"<code>__pydantic_post_init__</code>  <code>special</code>","text":"<p>The name of the post-init method for the model, if defined.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__pydantic_setattr_handlers__","title":"<code>__pydantic_setattr_handlers__</code>  <code>special</code>","text":"<p><code>__setattr__</code> handlers. Memoizing the handlers leads to a dramatic performance improvement in <code>__setattr__</code></p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.__signature__","title":"<code>__signature__</code>  <code>special</code>","text":"<p>The synthesized <code>__init__</code> [<code>Signature</code>][inspect.Signature] of the model.</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.model_config","title":"<code>model_config</code>","text":"<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.fit","title":"<code>fit(self, X, y, test_size=0.4, showTable=True)</code>","text":"<p>Main method of the QuantumClassifier class. Divides the input dataset in train and test according to the test_size parameter, creates and builds all the quantum models using the previously introduced parameters and trains them using X as training datapoints and y as target tags. </p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.fit--parameters","title":"Parameters","text":"<p>X : ndarray     Complete dataset values to be trained and fitted from. y : ndarray     Target tags for each dataset point for supervised learning. test_size : float, optional (default=0.4)     Floating point number between 0 and 1.0 that indicates which proportion of the dataset to be used to test the trained models. showTable : bool, optional (default=True)     If True, prints the table of results and accuracies in the terminal.</p> Source code in <code>lazyqml/lazyqml.py</code> <pre><code>def fit(self, X, y, test_size=0.4, showTable=True):\n    \"\"\"\n    Main method of the QuantumClassifier class. Divides the input dataset in train and test according to the test_size parameter, creates and builds all the quantum models using the previously introduced parameters and trains them using X as training datapoints and y as target tags. \n\n    Parameters\n    ----------\n    X : ndarray\n        Complete dataset values to be trained and fitted from.\n    y : ndarray\n        Target tags for each dataset point for supervised learning.\n    test_size : float, optional (default=0.4)\n        Floating point number between 0 and 1.0 that indicates which proportion of the dataset to be used to test the trained models.\n    showTable : bool, optional (default=True)\n        If True, prints the table of results and accuracies in the terminal.\n    \"\"\"\n\n    self._prepare_execution(X, y)\n\n    scores = self._dispatcher.dispatch(\n                    X=X,\n                    y=y,\n                    folds=1,\n                    repeats=1,\n                    mode=\"hold-out\",\n                    testsize=test_size,\n                    showTable=showTable\n                )\n\n    return scores\n</code></pre>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.leave_one_out","title":"<code>leave_one_out(self, X, y, showTable=True)</code>","text":"<p>Similar method to repeated_cross_validation. Carries out leave-one-out cross validation. Equivalent to repeated_cross_validation using n_splits=len(X) and n_repeats=1. </p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.leave_one_out--parameters","title":"Parameters","text":"<p>X : ndarray     Complete dataset values to be trained and fitted from. y : ndarray     Target tags for each dataset point for supervised learning. n_splits : int, optional (default=10)     Number of folds for k-fold cross validation training. n_repeats : int, optional (default=5)     Number of repetitions for k-fold cross validation. showTable : bool, optional (default=True)     If True, prints the table of results and accuracies in the terminal.</p> Source code in <code>lazyqml/lazyqml.py</code> <pre><code>def leave_one_out(self, X, y, showTable=True):\n    \"\"\"\n    Similar method to repeated_cross_validation. Carries out leave-one-out cross validation. Equivalent to repeated_cross_validation using n_splits=len(X) and n_repeats=1. \n\n    Parameters\n    ----------\n    X : ndarray\n        Complete dataset values to be trained and fitted from.\n    y : ndarray\n        Target tags for each dataset point for supervised learning.\n    n_splits : int, optional (default=10)\n        Number of folds for k-fold cross validation training.\n    n_repeats : int, optional (default=5)\n        Number of repetitions for k-fold cross validation.\n    showTable : bool, optional (default=True)\n        If True, prints the table of results and accuracies in the terminal.\n    \"\"\"\n    self._prepare_execution(X, y)\n\n    scores = self._dispatcher.dispatch(\n                    X=X,\n                    y=y,\n                    folds=len(X),\n                    repeats=1,\n                    mode=\"leave-one-out\",\n                    showTable=showTable\n                )\n\n    # No funcionaria porque hay que poner el modo dentro del dispatch\n    # self.repeated_cross_validation(X, y, len(X), 1, showTable)\n\n    return scores\n</code></pre>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.repeated_cross_validation","title":"<code>repeated_cross_validation(self, X, y, n_splits=10, n_repeats=5, showTable=True)</code>","text":"<p>Carries out k-fold cross validation based on n_splits (folds) and n_repeats (repeats). </p>"},{"location":"common/#lazyqml.lazyqml.QuantumClassifier.repeated_cross_validation--parameters","title":"Parameters","text":"<p>X : ndarray     Complete dataset values to be trained and fitted from. y : ndarray     Target tags for each dataset point for supervised learning. n_splits : int, optional (default=10)     Number of folds for k-fold cross validation training. n_repeats : int, optional (default=5)     Number of repetitions for k-fold cross validation. showTable : bool, optional (default=True)     If True, prints the table of results and accuracies in the terminal.</p> Source code in <code>lazyqml/lazyqml.py</code> <pre><code>def repeated_cross_validation(self, X, y, n_splits=10, n_repeats=5, showTable=True):\n    \"\"\"\n    Carries out k-fold cross validation based on n_splits (folds) and n_repeats (repeats). \n\n    Parameters\n    ----------\n    X : ndarray\n        Complete dataset values to be trained and fitted from.\n    y : ndarray\n        Target tags for each dataset point for supervised learning.\n    n_splits : int, optional (default=10)\n        Number of folds for k-fold cross validation training.\n    n_repeats : int, optional (default=5)\n        Number of repetitions for k-fold cross validation.\n    showTable : bool, optional (default=True)\n        If True, prints the table of results and accuracies in the terminal.\n    \"\"\"\n    self._prepare_execution(X, y)\n\n    scores = self._dispatcher.dispatch(\n                    X=X,\n                    y=y,\n                    folds=n_splits,\n                    repeats=n_repeats,\n                    mode=\"cross-validation\",\n                    showTable=showTable\n                )\n\n    return scores\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/QHPC-SP-Research-Lab/LazyQML/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>lazyqml could always use more documentation, whether as part of the official lazyqml docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/QHPC-SP-Research-Lab/LazyQML/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up lazyqml for local development.</p> <ol> <li> <p>Fork the lazyqml repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/lazyqml.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv lazyqml\n$ cd lazyqml/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 lazyqml tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/QHPC-SP-Research-Lab/LazyQML/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install lazyqml, run this command in your terminal:</p> <pre><code>pip install lazyqml\n</code></pre> <p>This is the preferred method to install lazyqml, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install lazyqml from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/QHPC-SP-Research-Lab/LazyQML.git\n</code></pre>"},{"location":"lazyqml/","title":"QuantumClassifier","text":""},{"location":"lazyqml/#quantumclassifier-parameters","title":"QuantumClassifier Parameters:","text":""},{"location":"lazyqml/#core-parameters","title":"Core Parameters:","text":"<ul> <li><code>nqubits</code>: <code>Set[int]</code></li> <li>Description: Set of qubit indices, where each value must be greater than 0.</li> <li> <p>Validation: Ensures that all elements are integers &gt; 0.</p> </li> <li> <p><code>randomstate</code>: <code>int</code></p> </li> <li>Description: Seed value for random number generation.</li> <li> <p>Default: <code>1234</code></p> </li> <li> <p><code>predictions</code>: <code>bool</code></p> </li> <li>Description: Flag to determine if predictions are enabled.</li> <li>Default: <code>False</code></li> </ul>"},{"location":"lazyqml/#model-structure-parameters","title":"Model Structure Parameters:","text":"<ul> <li><code>numPredictors</code>: <code>int</code></li> <li>Description: Number of predictors used in the QNN with bagging.</li> <li>Constraints: Must be greater than 0.</li> <li> <p>Default: <code>10</code></p> </li> <li> <p><code>numLayers</code>: <code>int</code></p> </li> <li>Description: Number of layers in the Quantum Neural Networks.</li> <li>Constraints: Must be greater than 0.</li> <li>Default: <code>5</code></li> </ul>"},{"location":"lazyqml/#set-based-configuration-parameters","title":"Set-Based Configuration Parameters:","text":"<ul> <li><code>classifiers</code>: <code>Set[Model]</code></li> <li>Description: Set of classifier models.</li> <li>Constraints: Must contain at least one classifier.</li> <li>Default: <code>{Model.ALL}</code></li> <li> <p>Options: <code>{Model.QNN, Model.QSVM, Model.QNN_BAG, Model.QKNN}</code></p> </li> <li> <p><code>ansatzs</code>: <code>Set[Ansatzs]</code></p> </li> <li>Description: Set of quantum ansatz configurations.</li> <li>Constraints: Must contain at least one ansatz.</li> <li>Default: <code>{Ansatzs.ALL}</code></li> <li> <p>Options: <code>{Ansatzs.RX, Ansatzs.RY, Ansatzs.RZ, Ansatzs.ZZ, Ansatzs.AMP, Ansatzs.DENSE_ANGLE, Ansatzs.HIGHER_ORDER}</code></p> </li> <li> <p><code>embeddings</code>: <code>Set[Embedding]</code></p> </li> <li>Description: Set of embedding strategies.</li> <li>Constraints: Must contain at least one embedding.</li> <li>Default: <code>{Embedding.ALL}</code></li> <li> <p>Options: <code>{Embedding.HCZRX, Embedding.TREE_TENSOR, Embedding.TWO_LOCAL, Embedding.HARDWARE_EFFICENT, Embedding.ANNULAR}</code></p> </li> <li> <p><code>features</code>: <code>Set[float]</code></p> </li> <li>Description: Set of feature values (must be between 0 and 1).</li> <li>Constraints: Values &gt; 0 and &lt;= 1.</li> <li>Default: <code>{0.3, 0.5, 0.8}</code></li> </ul>"},{"location":"lazyqml/#training-parameters","title":"Training Parameters:","text":"<ul> <li><code>learningRate</code>: <code>float</code></li> <li>Description: Learning rate for optimization.</li> <li>Constraints: Must be greater than 0.</li> <li> <p>Default: <code>0.01</code></p> </li> <li> <p><code>epochs</code>: <code>int</code></p> </li> <li>Description: Number of training epochs.</li> <li>Constraints: Must be greater than 0.</li> <li> <p>Default: <code>100</code></p> </li> <li> <p><code>batchSize</code>: <code>int</code></p> </li> <li>Description: Size of each batch during training.</li> <li>Constraints: Must be greater than 0.</li> <li>Default: <code>8</code></li> </ul>"},{"location":"lazyqml/#threshold-and-sampling","title":"Threshold and Sampling:","text":"<ul> <li><code>threshold</code>: <code>int</code></li> <li>Description: Decision threshold for parallelization, if the model is bigger than this threshold it will use GPU.</li> <li>Constraints: Must be greater than 0.</li> <li> <p>Default: <code>22</code></p> </li> <li> <p><code>maxSamples</code>: <code>float</code></p> </li> <li>Description: Maximum proportion of samples to be used from the dataset characteristics.</li> <li>Constraints: Between 0 and 1.</li> <li>Default: <code>1.0</code></li> </ul>"},{"location":"lazyqml/#logging-and-metrics","title":"Logging and Metrics:","text":"<ul> <li><code>verbose</code>: <code>bool</code></li> <li>Description: Flag for detailed output during training.</li> <li> <p>Default: <code>False</code></p> </li> <li> <p><code>customMetric</code>: <code>Optional[Callable]</code></p> </li> <li>Description: User-defined metric function for evaluation.</li> <li>Validation:<ul> <li>Function must accept <code>y_true</code> and <code>y_pred</code> as the first two arguments.</li> <li>Must return a scalar value (int or float).</li> <li>Function execution is validated with dummy arguments.</li> </ul> </li> <li>Default: <code>None</code></li> </ul>"},{"location":"lazyqml/#custom-preprocessors","title":"Custom Preprocessors:","text":"<ul> <li><code>customImputerNum</code>: <code>Optional[Any]</code></li> <li>Description: Custom numeric data imputer.</li> <li>Validation:<ul> <li>Must be an object with <code>fit</code>, <code>transform</code>, and optionally <code>fit_transform</code> methods.</li> <li>Validated with dummy data.</li> </ul> </li> <li> <p>Default: <code>None</code></p> </li> <li> <p><code>customImputerCat</code>: <code>Optional[Any]</code></p> </li> <li>Description: Custom categorical data imputer.</li> <li>Validation:<ul> <li>Must be an object with <code>fit</code>, <code>transform</code>, and optionally <code>fit_transform</code> methods.</li> <li>Validated with dummy data.</li> </ul> </li> <li>Default: <code>None</code></li> </ul>"},{"location":"lazyqml/#functions","title":"Functions:","text":""},{"location":"lazyqml/#fit","title":"<code>fit</code>","text":"<p><pre><code>fit(self, X, y, test_size=0.4, showTable=True)\n</code></pre> Fits classification algorithms to <code>X</code> and <code>y</code> using a hold-out approach. Predicts and scores on a test set determined by <code>test_size</code>.</p>"},{"location":"lazyqml/#parameters","title":"Parameters:","text":"<ul> <li><code>X</code>: Input features (DataFrame or compatible format).</li> <li><code>y</code>: Target labels (must be numeric, e.g., via <code>LabelEncoder</code> or <code>OrdinalEncoder</code>).</li> <li><code>test_size</code>: Proportion of the dataset to use as the test set. Default is <code>0.4</code>.</li> <li><code>showTable</code>: Display a table with results. Default is <code>True</code>.</li> </ul>"},{"location":"lazyqml/#behavior","title":"Behavior:","text":"<ul> <li>Validates the compatibility of input dimensions.</li> <li>Automatically applies PCA transformation for incompatible dimensions.</li> <li>Requires all categories to be present in training data.</li> </ul>"},{"location":"lazyqml/#repeated_cross_validation","title":"<code>repeated_cross_validation</code>","text":"<p><pre><code>repeated_cross_validation(self, X, y, n_splits=10, n_repeats=5, showTable=True)\n</code></pre> Performs repeated cross-validation on the dataset using the specified splits and repeats.</p>"},{"location":"lazyqml/#parameters_1","title":"Parameters:","text":"<ul> <li><code>X</code>: Input features (DataFrame or compatible format).</li> <li><code>y</code>: Target labels (must be numeric).</li> <li><code>n_splits</code>: Number of folds for splitting the dataset. Default is <code>10</code>.</li> <li><code>n_repeats</code>: Number of times cross-validation is repeated. Default is <code>5</code>.</li> <li><code>showTable</code>: Display a table with results. Default is <code>True</code>.</li> </ul>"},{"location":"lazyqml/#behavior_1","title":"Behavior:","text":"<ul> <li>Uses <code>RepeatedStratifiedKFold</code> for generating splits.</li> <li>Aggregates results from multiple train-test splits.</li> </ul>"},{"location":"lazyqml/#leave_one_out","title":"<code>leave_one_out</code>","text":"<p><pre><code>leave_one_out(self, X, y, showTable=True)\n</code></pre> Performs leave-one-out cross-validation on the dataset.</p>"},{"location":"lazyqml/#parameters_2","title":"Parameters:","text":"<ul> <li><code>X</code>: Input features (DataFrame or compatible format).</li> <li><code>y</code>: Target labels (must be numeric).</li> <li><code>showTable</code>: Display a table with results. Default is <code>True</code>.</li> </ul>"},{"location":"lazyqml/#behavior_2","title":"Behavior:","text":"<ul> <li>Uses <code>LeaveOneOut</code> for generating train-test splits.</li> <li>Evaluates the model on each split and aggregates results.</li> </ul>"},{"location":"usage/","title":"Usage","text":"<p>To use LazyQML:</p> <pre><code>from sklearn.datasets import load_iris\nfrom lazyqml import QuantumClassifier\n\n# Load data\ndata = load_iris()\nX = data.data\ny = data.target\n\nclassifier = QuantumClassifier(nqubits={4}, classifiers={Model.QNN, Model.QSVM}, epochs=10)\n\n# Fit and predict\nclassifier.fit(X=X, y=y, test_size=0.4)\n</code></pre>"}]}